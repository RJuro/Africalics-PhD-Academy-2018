{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training word2vec text8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RJuro/Africalics-PhD-Academy-2018/blob/master/notebooks/training_word2vec_text8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qWEUbS0b7rWZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training customized word embeddings\n",
        "\n",
        "Word embeddings became big around 2013 and are linked to [this paper](https://arxiv.org/abs/1301.3781) with the beautiful title \n",
        "*Efficient Estimation of Word Representations in Vector Space* by Tomas Mokolov et al. coming out og Google. This was the foundation of Word2Vec.\n",
        "\n",
        "The idea behind it is easiest summarized by the following quote: \n",
        "\n",
        "\n",
        "> *You shall know a word by the company it keeps (Firth, J. R. 1957:11)*\n",
        "\n",
        "Let me start with a fascinating example of word embeddings in practice. Below, you can see a figure from the paper: \n",
        "*Dynamic Word Embeddings for Evolving Semantic Discovery*. Here (in simple terms) the researchers estimated word vectors for from textual inputs in different time-frames. They picked out some terms and person that obviousely changed *their company* over the years. Then they look at the relative position of these terms compared to terms that did not change much (anchors). If you are interested in this kind of research, check out [this blog](https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/) that describes the paper briefly or the [original paper](https://arxiv.org/abs/1703.00607).\n",
        "\n",
        "![alt text](https://adriancolyer.files.wordpress.com/2018/02/evolving-word-embeddings-fig-1.jpeg)\n",
        "\n",
        "Word embeddings allow us to create term representations that \"learn\" meaning from semantic and syntactic features. These models take a sequence of sentences as an input and scan for all individual terms that appear in the whole corpus and all their occurrences. Such contextual learning seems to be able to pick up non-trivial conceptual details and it is this class of models that today enable technologies such as chatbots, machine translation and much more.\n",
        "\n",
        "The early word embedding models were Word2Vec and [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
        "In December 2017 Facebook presented [fastText](https://fasttext.cc/) (by the way - by 2017 Tomas Mikolov was working for Facebook and is one of the authors of the [paper](https://arxiv.org/abs/1607.04606) that introduces the research behind fastText). This model extends the idea of Word2Vec, enriching these vectors by information from sub-word elements. What does that mean? Words are not only defined by surrounding words but in addition also by the various sillables that make up the word. Why should that be a good idea? Well, now words such as *apple* and *apples* do not only get similar vectors due to them often sharing context but also because they are composed of the same sub-word elements. This comes in particularly handy when we are dealing with language that have a rich morphology such as Turkish or Russian.  This is also great, when working with web-text, which is often messy and misspelled.\n",
        "\n",
        "The current state-of-the-art (April 2018!) is ELMo (Embeddings from Language Models) that further tachkles the problem of contextuality and particularly polysemy i.e. same term means something else in a different context. \n",
        "\n",
        "You can read more about the ins and outs of the current state of  embedding models [here](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a).\n",
        "\n",
        "Now the good news: You will find pre-trained vectors from all mentioned models online. They will do great in most cases. However, when working with specific tasks: Some obscure languages and/or specific technical jargon (finance talk), it is nice to know how to train such word-vectors.\n",
        "\n",
        "In this tutorial and on M3 we will not go further than fastText (2017-state-of-the-art should be good enough for us â€“ sorry). You are more than welcome to use other, more sophisticated, embeddings.\n",
        "\n",
        "\n",
        "In this tutorial we will train 3 embedding models:\n",
        "\n",
        "- Word2Vec on text8 - a sample of English Wikipedia\n",
        "- Word2Vec on the hatespeech and toxic comments data\n",
        "- fastText on the toxic comments data\n",
        "\n",
        "Once trained, we will store the models\n"
      ]
    },
    {
      "metadata": {
        "id": "XJe4cEEVO3Kv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's start by installing Gensim that we know and love since M2\n",
        "\n",
        "\n",
        "#surpresses lots of output\n",
        "%%capture \n",
        "\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HrbC_yLZYK9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pandas for tabular data\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# import gensim and the Word2Vec as well as FastText models\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X671bfH0Slau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's downlaod all needed data (form different sources)\n",
        "%%capture \n",
        "\n",
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!wget https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv\n",
        "!wget http://sds-datacrunch.aau.dk/public/all.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OrqP_ALFS7_6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Also, we need to unzip text8 and our toxic-comments data\n",
        "%%capture \n",
        "\n",
        "!unzip text8.zip\n",
        "!unzip all.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6fD420QVkKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We import logging to get informative outputs from Gensim training\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNl_JViuRrVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec on text8\n",
        "\n",
        "in the following we will train the model step by step. In future, you can do things much faster\n",
        "\n",
        "- We first use the text8 wrapper to efficiently read the text8 file from disk. \n",
        "- Then we instantiate a Word2Vec model (with some parameters)\n",
        "- We build the vocabulary\n",
        "- Finally, we train it\n",
        "\n",
        "Once done, we can play a bit around with it."
      ]
    },
    {
      "metadata": {
        "id": "1q0jc0WPTDBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect to the text8 file on disk. Gensim has a number of wrappers for other filetypes and sources. \n",
        "# Obviopusely, for a many-gigabyte-wikipedia dump, you wouldn't load the whole thing into memory\n",
        "# But rather read it in line by line from disk.\n",
        "\n",
        "text8 = gensim.models.word2vec.Text8Corpus('text8', max_sentence_length=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8stKkrPyU82l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We instantiate the model. Words that appear less than 5 times are kicked out. \n",
        "# We will train over 3 iterations.\n",
        "\n",
        "model_text8 = Word2Vec(iter=3, min_count=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjytE7UmVMek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "98664ef1-29fe-4eb4-d179-6d5fe7ff6fda"
      },
      "cell_type": "code",
      "source": [
        "# Let's build the vocabulary\n",
        "\n",
        "model_text8.build_vocab(text8)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 20:39:08,094 : INFO : collecting all words and their counts\n",
            "2018-11-03 20:39:08,100 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2018-11-03 20:39:13,701 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
            "2018-11-03 20:39:13,702 : INFO : Loading a fresh vocabulary\n",
            "2018-11-03 20:39:14,447 : INFO : effective_min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
            "2018-11-03 20:39:14,449 : INFO : effective_min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
            "2018-11-03 20:39:14,684 : INFO : deleting the raw counts dictionary of 253854 items\n",
            "2018-11-03 20:39:14,697 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2018-11-03 20:39:14,700 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
            "2018-11-03 20:39:15,013 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
            "2018-11-03 20:39:15,014 : INFO : resetting layer weights\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4SI2VHyxVQ2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1598
        },
        "outputId": "72e9fea1-ddb4-4cbf-d4b3-56a2b6ad1126"
      },
      "cell_type": "code",
      "source": [
        "# Now we can start the training\n",
        "\n",
        "model_text8.train(text8, total_examples=model_text8.corpus_count, epochs=3)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 20:41:11,590 : INFO : training model with 3 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2018-11-03 20:41:12,609 : INFO : EPOCH 1 - PROGRESS: at 4.06% examples, 502655 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:13,635 : INFO : EPOCH 1 - PROGRESS: at 8.29% examples, 505036 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:41:14,658 : INFO : EPOCH 1 - PROGRESS: at 12.11% examples, 491411 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:15,679 : INFO : EPOCH 1 - PROGRESS: at 15.87% examples, 483510 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:41:16,683 : INFO : EPOCH 1 - PROGRESS: at 19.69% examples, 481828 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:17,690 : INFO : EPOCH 1 - PROGRESS: at 23.34% examples, 477614 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:18,716 : INFO : EPOCH 1 - PROGRESS: at 27.16% examples, 476481 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:19,738 : INFO : EPOCH 1 - PROGRESS: at 30.86% examples, 474388 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:20,753 : INFO : EPOCH 1 - PROGRESS: at 34.57% examples, 473091 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:21,759 : INFO : EPOCH 1 - PROGRESS: at 38.33% examples, 472661 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:22,763 : INFO : EPOCH 1 - PROGRESS: at 41.92% examples, 470437 words/s, in_qsize 6, out_qsize 2\n",
            "2018-11-03 20:41:23,801 : INFO : EPOCH 1 - PROGRESS: at 45.74% examples, 469807 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:24,815 : INFO : EPOCH 1 - PROGRESS: at 49.50% examples, 469576 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:25,821 : INFO : EPOCH 1 - PROGRESS: at 53.20% examples, 469003 words/s, in_qsize 4, out_qsize 0\n",
            "2018-11-03 20:41:26,824 : INFO : EPOCH 1 - PROGRESS: at 56.91% examples, 468707 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:27,846 : INFO : EPOCH 1 - PROGRESS: at 60.61% examples, 467875 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:41:28,860 : INFO : EPOCH 1 - PROGRESS: at 64.37% examples, 467696 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:29,862 : INFO : EPOCH 1 - PROGRESS: at 68.14% examples, 467843 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:30,870 : INFO : EPOCH 1 - PROGRESS: at 71.84% examples, 467446 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:31,872 : INFO : EPOCH 1 - PROGRESS: at 75.54% examples, 466902 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:32,875 : INFO : EPOCH 1 - PROGRESS: at 79.31% examples, 466539 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:33,877 : INFO : EPOCH 1 - PROGRESS: at 83.07% examples, 466584 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:34,893 : INFO : EPOCH 1 - PROGRESS: at 86.83% examples, 466382 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:35,894 : INFO : EPOCH 1 - PROGRESS: at 90.53% examples, 466398 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:36,912 : INFO : EPOCH 1 - PROGRESS: at 94.36% examples, 466296 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:37,920 : INFO : EPOCH 1 - PROGRESS: at 98.00% examples, 465647 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:38,414 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 20:41:38,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 20:41:38,436 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 20:41:38,437 : INFO : EPOCH - 1 : training on 17005207 raw words (12505540 effective words) took 26.8s, 465939 effective words/s\n",
            "2018-11-03 20:41:39,443 : INFO : EPOCH 2 - PROGRESS: at 3.64% examples, 456821 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:40,459 : INFO : EPOCH 2 - PROGRESS: at 7.52% examples, 463208 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:41,467 : INFO : EPOCH 2 - PROGRESS: at 11.29% examples, 462596 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:42,469 : INFO : EPOCH 2 - PROGRESS: at 14.93% examples, 460687 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:43,479 : INFO : EPOCH 2 - PROGRESS: at 18.69% examples, 461920 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:44,482 : INFO : EPOCH 2 - PROGRESS: at 22.46% examples, 463048 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:45,518 : INFO : EPOCH 2 - PROGRESS: at 26.22% examples, 462497 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:46,531 : INFO : EPOCH 2 - PROGRESS: at 29.98% examples, 463596 words/s, in_qsize 6, out_qsize 0\n",
            "2018-11-03 20:41:47,556 : INFO : EPOCH 2 - PROGRESS: at 33.69% examples, 462808 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:41:48,565 : INFO : EPOCH 2 - PROGRESS: at 37.39% examples, 462838 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:49,585 : INFO : EPOCH 2 - PROGRESS: at 41.15% examples, 462698 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:41:50,608 : INFO : EPOCH 2 - PROGRESS: at 44.91% examples, 462675 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:51,618 : INFO : EPOCH 2 - PROGRESS: at 48.68% examples, 463098 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:52,642 : INFO : EPOCH 2 - PROGRESS: at 52.38% examples, 462303 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:41:53,659 : INFO : EPOCH 2 - PROGRESS: at 56.14% examples, 462698 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:54,674 : INFO : EPOCH 2 - PROGRESS: at 59.85% examples, 462302 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:55,688 : INFO : EPOCH 2 - PROGRESS: at 63.61% examples, 462477 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:56,707 : INFO : EPOCH 2 - PROGRESS: at 67.37% examples, 462405 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:57,736 : INFO : EPOCH 2 - PROGRESS: at 71.19% examples, 462675 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:58,738 : INFO : EPOCH 2 - PROGRESS: at 74.96% examples, 462948 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:41:59,766 : INFO : EPOCH 2 - PROGRESS: at 78.78% examples, 462352 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:00,771 : INFO : EPOCH 2 - PROGRESS: at 82.42% examples, 461835 words/s, in_qsize 6, out_qsize 2\n",
            "2018-11-03 20:42:01,786 : INFO : EPOCH 2 - PROGRESS: at 86.07% examples, 461296 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:02,793 : INFO : EPOCH 2 - PROGRESS: at 89.77% examples, 461327 words/s, in_qsize 6, out_qsize 0\n",
            "2018-11-03 20:42:03,800 : INFO : EPOCH 2 - PROGRESS: at 93.47% examples, 461061 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:04,804 : INFO : EPOCH 2 - PROGRESS: at 97.18% examples, 460988 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:05,525 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 20:42:05,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 20:42:05,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 20:42:05,534 : INFO : EPOCH - 2 : training on 17005207 raw words (12502350 effective words) took 27.1s, 461490 effective words/s\n",
            "2018-11-03 20:42:06,556 : INFO : EPOCH 3 - PROGRESS: at 3.64% examples, 450623 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:07,563 : INFO : EPOCH 3 - PROGRESS: at 7.47% examples, 458676 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:08,570 : INFO : EPOCH 3 - PROGRESS: at 11.17% examples, 457369 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:09,575 : INFO : EPOCH 3 - PROGRESS: at 14.76% examples, 454703 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:10,588 : INFO : EPOCH 3 - PROGRESS: at 18.58% examples, 458371 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:11,591 : INFO : EPOCH 3 - PROGRESS: at 22.28% examples, 458872 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:12,593 : INFO : EPOCH 3 - PROGRESS: at 25.87% examples, 458172 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:13,600 : INFO : EPOCH 3 - PROGRESS: at 29.51% examples, 458082 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:14,608 : INFO : EPOCH 3 - PROGRESS: at 33.16% examples, 458132 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:15,620 : INFO : EPOCH 3 - PROGRESS: at 36.80% examples, 457793 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:16,651 : INFO : EPOCH 3 - PROGRESS: at 40.56% examples, 457599 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:17,668 : INFO : EPOCH 3 - PROGRESS: at 44.33% examples, 458160 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:18,684 : INFO : EPOCH 3 - PROGRESS: at 48.09% examples, 458842 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:19,686 : INFO : EPOCH 3 - PROGRESS: at 51.91% examples, 460143 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:20,696 : INFO : EPOCH 3 - PROGRESS: at 55.61% examples, 460345 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:21,702 : INFO : EPOCH 3 - PROGRESS: at 59.38% examples, 460848 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:22,705 : INFO : EPOCH 3 - PROGRESS: at 63.08% examples, 460946 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:23,723 : INFO : EPOCH 3 - PROGRESS: at 66.90% examples, 461447 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:24,732 : INFO : EPOCH 3 - PROGRESS: at 70.61% examples, 461444 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:25,738 : INFO : EPOCH 3 - PROGRESS: at 74.31% examples, 461529 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:26,746 : INFO : EPOCH 3 - PROGRESS: at 78.25% examples, 461979 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:27,757 : INFO : EPOCH 3 - PROGRESS: at 81.95% examples, 461720 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:28,777 : INFO : EPOCH 3 - PROGRESS: at 85.71% examples, 461736 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-03 20:42:29,780 : INFO : EPOCH 3 - PROGRESS: at 89.42% examples, 461733 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-03 20:42:30,783 : INFO : EPOCH 3 - PROGRESS: at 93.12% examples, 461653 words/s, in_qsize 6, out_qsize 2\n",
            "2018-11-03 20:42:31,796 : INFO : EPOCH 3 - PROGRESS: at 96.88% examples, 461671 words/s, in_qsize 5, out_qsize 1\n",
            "2018-11-03 20:42:32,586 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 20:42:32,599 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 20:42:32,609 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 20:42:32,610 : INFO : EPOCH - 3 : training on 17005207 raw words (12506606 effective words) took 27.1s, 462044 effective words/s\n",
            "2018-11-03 20:42:32,612 : INFO : training on a 51015621 raw words (37514496 effective words) took 81.0s, 463024 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37514496, 51015621)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "ifyLsuUQXdWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "76b1ce9d-783a-4d59-c3ab-2ef129d60520"
      },
      "cell_type": "code",
      "source": [
        "# Training is done and we can play a bit around\n",
        "\n",
        "# ask the model for most similar word to some term\n",
        "model_text8.wv.most_similar(\"milk\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 20:43:18,152 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('meat', 0.9049235582351685),\n",
              " ('fruit', 0.904716432094574),\n",
              " ('honey', 0.8918293714523315),\n",
              " ('beans', 0.8837248086929321),\n",
              " ('sugar', 0.8805627822875977),\n",
              " ('vegetables', 0.8805302381515503),\n",
              " ('beef', 0.8631115555763245),\n",
              " ('vegetable', 0.8613918423652649),\n",
              " ('drinks', 0.8598768711090088),\n",
              " ('chocolate', 0.8590322732925415)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "X5wYqOJ_VolR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b6989cf6-a289-48e3-db5a-2d091e77c117"
      },
      "cell_type": "code",
      "source": [
        "# Do some algebra exercises\n",
        "\n",
        "model_text8.wv.most_similar(positive=['germany', 'paris'], negative=['france'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('berlin', 0.8118886351585388),\n",
              " ('vienna', 0.7582015991210938),\n",
              " ('munich', 0.7417254447937012),\n",
              " ('moscow', 0.665905237197876),\n",
              " ('leipzig', 0.6623905897140503),\n",
              " ('milan', 0.6177572011947632),\n",
              " ('hamburg', 0.6133787631988525),\n",
              " ('frankfurt', 0.6034635305404663),\n",
              " ('bologna', 0.5932918190956116),\n",
              " ('bonn', 0.5911003351211548)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "nXuKixFIUf2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "1cee60cb-9363-4850-9933-ebd6eda7d98b"
      },
      "cell_type": "code",
      "source": [
        "model_text8.wv.most_similar(positive=['japan', 'mercedes'], negative=['germany'])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('honda', 0.7667422294616699),\n",
              " ('sony', 0.7596989870071411),\n",
              " ('jeep', 0.7313293814659119),\n",
              " ('toyota', 0.7283535003662109),\n",
              " ('nintendo', 0.7144696712493896),\n",
              " ('mitsubishi', 0.7099435329437256),\n",
              " ('bmw', 0.7031159996986389),\n",
              " ('mazda', 0.6964906454086304),\n",
              " ('jaguar', 0.6929983496665955),\n",
              " ('motorcycle', 0.6928412914276123)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "T9XqFJyDrXCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "dc8ccaf3-3194-4623-bae9-5788a7124eb2"
      },
      "cell_type": "code",
      "source": [
        "# and more\n",
        "\n",
        "print(model_text8.wv.doesnt_match(['dog','cat','chicken','hamster']))\n",
        "print()\n",
        "print(model_text8.wv.doesnt_match(['bus','street','honey','house']))\n",
        "print()\n",
        "print(model_text8.wv.doesnt_match(['gin','vodka','beer','whiskey']))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chicken\n",
            "\n",
            "honey\n",
            "\n",
            "beer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fqltZz07VTOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec on a specific copus (form memory)\n",
        "\n",
        "In the following we will train the model first on the toxic-comments data from [this Kaggle challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) \n",
        "\n",
        "Then, we will re-train the model with more data (yes, that's possible, since the Word2Vec model is actually a shallow neural network). Here we will use the hate-speech classification tweets"
      ]
    },
    {
      "metadata": {
        "id": "7uEBNblVYDOL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load up the toxic comments data\n",
        "\n",
        "toxic1 = pd.read_csv('train.csv')\n",
        "toxic2 = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VVz4s4F2lkCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load up the hatespeech tweets\n",
        "\n",
        "hatespeech_tweets = pd.read_csv('labeled_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wuvyQjCzXLo9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### some light preprocessing\n",
        "\n",
        "we will use twitter-style preprocessing to get rid of typical online-text issues."
      ]
    },
    {
      "metadata": {
        "id": "nkUGqUIFYQTJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import tweet-tokenizer\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxH9KZTEXX6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocess the toxic comments text\n",
        "# just tokenizing, loweing and removing everything that is not a word\n",
        "# given 2 files, we are doing it 2 times\n",
        "\n",
        "toxic1['tokenized'] = toxic1['comment_text'].map(lambda t: tknzr.tokenize(t))\n",
        "toxic1['tokenized'] = toxic1['tokenized'].map(lambda t: [x.lower() for x in t if x.isalpha()])\n",
        "\n",
        "toxic2['tokenized'] = toxic2['comment_text'].map(lambda t: tknzr.tokenize(t))\n",
        "toxic2['tokenized'] = toxic2['tokenized'].map(lambda t: [x.lower() for x in t if x.isalpha()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4giQAuAuXsGN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Finally, we can just concatenate the tokenized colum of both dataframes as out training data input\n",
        "\n",
        "toxic_data = pd.concat([toxic1['tokenized'],toxic2['tokenized']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MTWfkk85mRqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Same preprocessing for the hatespeech tweets.\n",
        "\n",
        "hatespeech_tweets['tokenized'] = hatespeech_tweets['tweet'].map(lambda t: tknzr.tokenize(t))\n",
        "hatespeech_tweets['tokenized'] = hatespeech_tweets['tokenized'].map(lambda t: [x.lower() for x in t if x.isalpha()])\n",
        "\n",
        "### DONE ####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OcGpxuVpYi-w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model training in one line\n",
        "\n",
        "We can actually instantiate, build vocabulary and train in one line only. Isn't that great?"
      ]
    },
    {
      "metadata": {
        "id": "5eSBBgKpZUeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2346
        },
        "outputId": "932e657b-c8b9-43bf-84f5-96b4a98a4de4"
      },
      "cell_type": "code",
      "source": [
        "# We can instantiate and train the model in one line. Just pass the input data (sequence of token-lists)\n",
        "# specify target dimensionality (size), the window adound the target term, minimum count, and number of iterations/epochs\n",
        "# workers are optional (for multiprocessing)\n",
        "\n",
        "model_toxic = Word2Vec(toxic_data, size=100, window=5, min_count=5, workers=4, iter=3)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 21:03:01,790 : INFO : collecting all words and their counts\n",
            "2018-11-03 21:03:01,814 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2018-11-03 21:03:01,977 : INFO : PROGRESS: at sentence #10000, processed 655155 words, keeping 33301 word types\n",
            "2018-11-03 21:03:02,150 : INFO : PROGRESS: at sentence #20000, processed 1297718 words, keeping 49193 word types\n",
            "2018-11-03 21:03:02,318 : INFO : PROGRESS: at sentence #30000, processed 1928609 words, keeping 61707 word types\n",
            "2018-11-03 21:03:02,494 : INFO : PROGRESS: at sentence #40000, processed 2584346 words, keeping 72823 word types\n",
            "2018-11-03 21:03:02,669 : INFO : PROGRESS: at sentence #50000, processed 3219612 words, keeping 82595 word types\n",
            "2018-11-03 21:03:02,864 : INFO : PROGRESS: at sentence #60000, processed 3887527 words, keeping 91889 word types\n",
            "2018-11-03 21:03:03,043 : INFO : PROGRESS: at sentence #70000, processed 4534434 words, keeping 100497 word types\n",
            "2018-11-03 21:03:03,219 : INFO : PROGRESS: at sentence #80000, processed 5180185 words, keeping 108491 word types\n",
            "2018-11-03 21:03:03,392 : INFO : PROGRESS: at sentence #90000, processed 5808163 words, keeping 115793 word types\n",
            "2018-11-03 21:03:03,571 : INFO : PROGRESS: at sentence #100000, processed 6446667 words, keeping 122921 word types\n",
            "2018-11-03 21:03:03,758 : INFO : PROGRESS: at sentence #110000, processed 7104669 words, keeping 129843 word types\n",
            "2018-11-03 21:03:03,932 : INFO : PROGRESS: at sentence #120000, processed 7727707 words, keeping 136425 word types\n",
            "2018-11-03 21:03:04,110 : INFO : PROGRESS: at sentence #130000, processed 8366226 words, keeping 142670 word types\n",
            "2018-11-03 21:03:04,290 : INFO : PROGRESS: at sentence #140000, processed 9013610 words, keeping 149016 word types\n",
            "2018-11-03 21:03:04,462 : INFO : PROGRESS: at sentence #150000, processed 9662740 words, keeping 155180 word types\n",
            "2018-11-03 21:03:04,627 : INFO : PROGRESS: at sentence #160000, processed 10300877 words, keeping 161216 word types\n",
            "2018-11-03 21:03:04,784 : INFO : PROGRESS: at sentence #170000, processed 10890470 words, keeping 172480 word types\n",
            "2018-11-03 21:03:04,956 : INFO : PROGRESS: at sentence #180000, processed 11476476 words, keeping 183846 word types\n",
            "2018-11-03 21:03:05,109 : INFO : PROGRESS: at sentence #190000, processed 12058283 words, keeping 193102 word types\n",
            "2018-11-03 21:03:05,265 : INFO : PROGRESS: at sentence #200000, processed 12641348 words, keeping 203033 word types\n",
            "2018-11-03 21:03:05,419 : INFO : PROGRESS: at sentence #210000, processed 13214148 words, keeping 212858 word types\n",
            "2018-11-03 21:03:05,575 : INFO : PROGRESS: at sentence #220000, processed 13801021 words, keeping 221810 word types\n",
            "2018-11-03 21:03:05,731 : INFO : PROGRESS: at sentence #230000, processed 14386389 words, keeping 231411 word types\n",
            "2018-11-03 21:03:05,888 : INFO : PROGRESS: at sentence #240000, processed 14970014 words, keeping 242788 word types\n",
            "2018-11-03 21:03:06,046 : INFO : PROGRESS: at sentence #250000, processed 15552888 words, keeping 251686 word types\n",
            "2018-11-03 21:03:06,200 : INFO : PROGRESS: at sentence #260000, processed 16128355 words, keeping 259947 word types\n",
            "2018-11-03 21:03:06,355 : INFO : PROGRESS: at sentence #270000, processed 16705792 words, keeping 267681 word types\n",
            "2018-11-03 21:03:06,516 : INFO : PROGRESS: at sentence #280000, processed 17294130 words, keeping 277016 word types\n",
            "2018-11-03 21:03:06,675 : INFO : PROGRESS: at sentence #290000, processed 17875512 words, keeping 284992 word types\n",
            "2018-11-03 21:03:06,826 : INFO : PROGRESS: at sentence #300000, processed 18445270 words, keeping 292731 word types\n",
            "2018-11-03 21:03:06,987 : INFO : PROGRESS: at sentence #310000, processed 19041697 words, keeping 301325 word types\n",
            "2018-11-03 21:03:07,042 : INFO : collected 303684 word types from a corpus of 19210184 raw words and 312735 sentences\n",
            "2018-11-03 21:03:07,043 : INFO : Loading a fresh vocabulary\n",
            "2018-11-03 21:03:07,323 : INFO : effective_min_count=5 retains 62352 unique words (20% of original 303684, drops 241332)\n",
            "2018-11-03 21:03:07,324 : INFO : effective_min_count=5 leaves 18856477 word corpus (98% of original 19210184, drops 353707)\n",
            "2018-11-03 21:03:07,526 : INFO : deleting the raw counts dictionary of 303684 items\n",
            "2018-11-03 21:03:07,537 : INFO : sample=0.001 downsamples 51 most-common words\n",
            "2018-11-03 21:03:07,539 : INFO : downsampling leaves estimated 14286417 word corpus (75.8% of prior 18856477)\n",
            "2018-11-03 21:03:07,825 : INFO : estimated required memory for 62352 words and 100 dimensions: 81057600 bytes\n",
            "2018-11-03 21:03:07,826 : INFO : resetting layer weights\n",
            "2018-11-03 21:03:08,481 : INFO : training model with 4 workers on 62352 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2018-11-03 21:03:09,556 : INFO : EPOCH 1 - PROGRESS: at 3.56% examples, 515513 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:03:10,573 : INFO : EPOCH 1 - PROGRESS: at 7.31% examples, 528883 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:11,585 : INFO : EPOCH 1 - PROGRESS: at 10.93% examples, 531959 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:12,587 : INFO : EPOCH 1 - PROGRESS: at 14.55% examples, 532944 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:13,597 : INFO : EPOCH 1 - PROGRESS: at 18.09% examples, 534149 words/s, in_qsize 5, out_qsize 2\n",
            "2018-11-03 21:03:14,618 : INFO : EPOCH 1 - PROGRESS: at 21.76% examples, 536194 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:15,646 : INFO : EPOCH 1 - PROGRESS: at 25.46% examples, 536360 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:16,670 : INFO : EPOCH 1 - PROGRESS: at 29.27% examples, 537527 words/s, in_qsize 7, out_qsize 1\n",
            "2018-11-03 21:03:17,674 : INFO : EPOCH 1 - PROGRESS: at 32.97% examples, 538866 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:18,679 : INFO : EPOCH 1 - PROGRESS: at 36.53% examples, 538309 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:19,710 : INFO : EPOCH 1 - PROGRESS: at 40.38% examples, 538686 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:20,734 : INFO : EPOCH 1 - PROGRESS: at 44.08% examples, 539148 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:21,734 : INFO : EPOCH 1 - PROGRESS: at 47.75% examples, 539994 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:03:22,751 : INFO : EPOCH 1 - PROGRESS: at 51.32% examples, 538613 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:23,752 : INFO : EPOCH 1 - PROGRESS: at 55.34% examples, 539503 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:24,757 : INFO : EPOCH 1 - PROGRESS: at 59.37% examples, 539212 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:03:25,768 : INFO : EPOCH 1 - PROGRESS: at 63.27% examples, 538790 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:26,778 : INFO : EPOCH 1 - PROGRESS: at 67.28% examples, 538359 words/s, in_qsize 8, out_qsize 1\n",
            "2018-11-03 21:03:27,818 : INFO : EPOCH 1 - PROGRESS: at 71.32% examples, 537981 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:28,828 : INFO : EPOCH 1 - PROGRESS: at 75.25% examples, 537649 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:29,842 : INFO : EPOCH 1 - PROGRESS: at 79.36% examples, 538297 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:03:30,878 : INFO : EPOCH 1 - PROGRESS: at 83.48% examples, 537762 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:31,914 : INFO : EPOCH 1 - PROGRESS: at 87.64% examples, 537898 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:32,915 : INFO : EPOCH 1 - PROGRESS: at 91.65% examples, 538439 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:33,927 : INFO : EPOCH 1 - PROGRESS: at 95.74% examples, 538471 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:34,936 : INFO : EPOCH 1 - PROGRESS: at 99.65% examples, 538320 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:35,001 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-11-03 21:03:35,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 21:03:35,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 21:03:35,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 21:03:35,017 : INFO : EPOCH - 1 : training on 19210184 raw words (14286411 effective words) took 26.5s, 538784 effective words/s\n",
            "2018-11-03 21:03:36,054 : INFO : EPOCH 2 - PROGRESS: at 3.52% examples, 530125 words/s, in_qsize 7, out_qsize 1\n",
            "2018-11-03 21:03:37,056 : INFO : EPOCH 2 - PROGRESS: at 7.26% examples, 538840 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:38,094 : INFO : EPOCH 2 - PROGRESS: at 10.94% examples, 536409 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:39,097 : INFO : EPOCH 2 - PROGRESS: at 14.67% examples, 539796 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:40,103 : INFO : EPOCH 2 - PROGRESS: at 18.22% examples, 541503 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:41,114 : INFO : EPOCH 2 - PROGRESS: at 21.81% examples, 540839 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:42,132 : INFO : EPOCH 2 - PROGRESS: at 25.51% examples, 541057 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:43,135 : INFO : EPOCH 2 - PROGRESS: at 29.18% examples, 540231 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:44,136 : INFO : EPOCH 2 - PROGRESS: at 32.83% examples, 540717 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:03:45,141 : INFO : EPOCH 2 - PROGRESS: at 36.44% examples, 540628 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:46,142 : INFO : EPOCH 2 - PROGRESS: at 40.21% examples, 541616 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:47,155 : INFO : EPOCH 2 - PROGRESS: at 43.85% examples, 541176 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:48,160 : INFO : EPOCH 2 - PROGRESS: at 47.56% examples, 542289 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:49,179 : INFO : EPOCH 2 - PROGRESS: at 51.33% examples, 542654 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:50,184 : INFO : EPOCH 2 - PROGRESS: at 55.27% examples, 542691 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:51,194 : INFO : EPOCH 2 - PROGRESS: at 59.37% examples, 542705 words/s, in_qsize 7, out_qsize 1\n",
            "2018-11-03 21:03:52,198 : INFO : EPOCH 2 - PROGRESS: at 63.37% examples, 542919 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:53,217 : INFO : EPOCH 2 - PROGRESS: at 67.47% examples, 542811 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:54,230 : INFO : EPOCH 2 - PROGRESS: at 71.56% examples, 542954 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:55,236 : INFO : EPOCH 2 - PROGRESS: at 75.53% examples, 542813 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:56,239 : INFO : EPOCH 2 - PROGRESS: at 79.54% examples, 542858 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:57,243 : INFO : EPOCH 2 - PROGRESS: at 83.65% examples, 542891 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:58,256 : INFO : EPOCH 2 - PROGRESS: at 87.70% examples, 542707 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:03:59,282 : INFO : EPOCH 2 - PROGRESS: at 91.65% examples, 542184 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:00,282 : INFO : EPOCH 2 - PROGRESS: at 95.69% examples, 542007 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:01,297 : INFO : EPOCH 2 - PROGRESS: at 99.59% examples, 541620 words/s, in_qsize 8, out_qsize 1\n",
            "2018-11-03 21:04:01,359 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-11-03 21:04:01,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 21:04:01,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 21:04:01,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 21:04:01,392 : INFO : EPOCH - 2 : training on 19210184 raw words (14286769 effective words) took 26.4s, 542067 effective words/s\n",
            "2018-11-03 21:04:02,418 : INFO : EPOCH 3 - PROGRESS: at 3.42% examples, 517130 words/s, in_qsize 8, out_qsize 1\n",
            "2018-11-03 21:04:03,430 : INFO : EPOCH 3 - PROGRESS: at 7.15% examples, 531656 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:04:04,441 : INFO : EPOCH 3 - PROGRESS: at 10.88% examples, 538639 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:05,458 : INFO : EPOCH 3 - PROGRESS: at 14.67% examples, 541617 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:06,460 : INFO : EPOCH 3 - PROGRESS: at 18.22% examples, 543378 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:04:07,466 : INFO : EPOCH 3 - PROGRESS: at 21.85% examples, 544085 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:08,466 : INFO : EPOCH 3 - PROGRESS: at 25.60% examples, 546151 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:09,471 : INFO : EPOCH 3 - PROGRESS: at 29.26% examples, 544538 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:10,484 : INFO : EPOCH 3 - PROGRESS: at 32.74% examples, 540659 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:04:11,506 : INFO : EPOCH 3 - PROGRESS: at 36.10% examples, 536798 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:12,536 : INFO : EPOCH 3 - PROGRESS: at 39.77% examples, 534787 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:13,538 : INFO : EPOCH 3 - PROGRESS: at 43.27% examples, 533527 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:14,558 : INFO : EPOCH 3 - PROGRESS: at 46.70% examples, 531255 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:04:15,599 : INFO : EPOCH 3 - PROGRESS: at 50.23% examples, 529498 words/s, in_qsize 7, out_qsize 1\n",
            "2018-11-03 21:04:16,602 : INFO : EPOCH 3 - PROGRESS: at 53.84% examples, 528012 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:17,604 : INFO : EPOCH 3 - PROGRESS: at 57.51% examples, 526243 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:18,626 : INFO : EPOCH 3 - PROGRESS: at 61.17% examples, 523743 words/s, in_qsize 6, out_qsize 1\n",
            "2018-11-03 21:04:19,638 : INFO : EPOCH 3 - PROGRESS: at 65.01% examples, 522873 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:20,659 : INFO : EPOCH 3 - PROGRESS: at 68.96% examples, 523449 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:21,699 : INFO : EPOCH 3 - PROGRESS: at 72.86% examples, 522838 words/s, in_qsize 7, out_qsize 2\n",
            "2018-11-03 21:04:22,721 : INFO : EPOCH 3 - PROGRESS: at 76.90% examples, 523196 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:23,753 : INFO : EPOCH 3 - PROGRESS: at 80.92% examples, 523398 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:24,800 : INFO : EPOCH 3 - PROGRESS: at 85.03% examples, 523295 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:04:25,832 : INFO : EPOCH 3 - PROGRESS: at 89.06% examples, 523741 words/s, in_qsize 4, out_qsize 3\n",
            "2018-11-03 21:04:26,866 : INFO : EPOCH 3 - PROGRESS: at 93.19% examples, 524503 words/s, in_qsize 7, out_qsize 0\n",
            "2018-11-03 21:04:27,868 : INFO : EPOCH 3 - PROGRESS: at 97.33% examples, 525763 words/s, in_qsize 8, out_qsize 0\n",
            "2018-11-03 21:04:28,542 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-11-03 21:04:28,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 21:04:28,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 21:04:28,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 21:04:28,562 : INFO : EPOCH - 3 : training on 19210184 raw words (14286587 effective words) took 27.2s, 526166 effective words/s\n",
            "2018-11-03 21:04:28,563 : INFO : training on a 57630552 raw words (42859767 effective words) took 80.1s, 535204 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-Qs7pT8ZqIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "21631617-aa99-4268-ff5c-fdef534b605e"
      },
      "cell_type": "code",
      "source": [
        "# Does it work? The output of this cell makes it clear: Yes, it works pretty well.\n",
        "\n",
        "model_toxic.wv.most_similar('idiot')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 21:06:12,882 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('asshole', 0.8236082196235657),\n",
              " ('moron', 0.7038889527320862),\n",
              " ('imbecile', 0.6953639388084412),\n",
              " ('fool', 0.6788172721862793),\n",
              " ('loser', 0.6326109170913696),\n",
              " ('illiterate', 0.6308602094650269),\n",
              " ('ignorant', 0.6303929090499878),\n",
              " ('hypocrite', 0.6218823194503784),\n",
              " ('retard', 0.6214895248413086),\n",
              " ('insult', 0.6125708818435669)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "dCwnRH6SZ3Cq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "56e07e94-dd0e-4be9-f806-ff9626a8988d"
      },
      "cell_type": "code",
      "source": [
        "# something less violent?\n",
        "\n",
        "model_toxic.wv.most_similar('mother')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('father', 0.8239043354988098),\n",
              " ('wife', 0.8019334077835083),\n",
              " ('grandmother', 0.7894612550735474),\n",
              " ('mom', 0.7853889465332031),\n",
              " ('sister', 0.7848141193389893),\n",
              " ('dad', 0.7723446488380432),\n",
              " ('tongue', 0.7573986649513245),\n",
              " ('daughter', 0.7491512298583984),\n",
              " ('boyfriend', 0.745460033416748),\n",
              " ('parents', 0.7441803812980652)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "ze3r1ABfZ5M6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1c244c4d-5dca-4e2b-f0b8-c42ebfd75585"
      },
      "cell_type": "code",
      "source": [
        "model_toxic.save('model.m')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 21:08:23,499 : INFO : saving Word2Vec object under model.m, separately None\n",
            "2018-11-03 21:08:23,501 : INFO : not storing attribute vectors_norm\n",
            "2018-11-03 21:08:23,504 : INFO : not storing attribute cum_table\n",
            "2018-11-03 21:08:24,328 : INFO : saved model.m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xWjYZ9JLZqQe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now imagine, you trained a model and would like to update it with new data later. \n",
        "\n",
        "You can save your model for later re-use just like that:\n",
        "\n",
        "```\n",
        "model_toxic.save('model.m')\n",
        "```\n",
        "\n",
        "To re-train, we need to \n",
        "- build some additional vocab (perhaps the new data contains some new words)\n",
        "- train the model using the ```train``` method. (surprise)\n"
      ]
    },
    {
      "metadata": {
        "id": "ZTuvyxiGu_3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "dada2af4-98fd-449e-ba9d-1c314985b50b"
      },
      "cell_type": "code",
      "source": [
        "# Build ne vocab from new data, telling the model that it's an update\n",
        "\n",
        "model_toxic.build_vocab(hatespeech_tweets['tokenized'], update=True)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 21:11:09,890 : INFO : collecting all words and their counts\n",
            "2018-11-03 21:11:09,893 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2018-11-03 21:11:09,927 : INFO : PROGRESS: at sentence #10000, processed 111788 words, keeping 10979 word types\n",
            "2018-11-03 21:11:09,968 : INFO : PROGRESS: at sentence #20000, processed 247332 words, keeping 16372 word types\n",
            "2018-11-03 21:11:09,986 : INFO : collected 18336 word types from a corpus of 305428 raw words and 24783 sentences\n",
            "2018-11-03 21:11:09,988 : INFO : Updating model with new vocabulary\n",
            "2018-11-03 21:11:10,007 : INFO : New added 4191 unique words (18% of original 22527) and increased the count of 4191 pre-existing words (18% of original 22527)\n",
            "2018-11-03 21:11:10,038 : INFO : deleting the raw counts dictionary of 18336 items\n",
            "2018-11-03 21:11:10,040 : INFO : sample=0.001 downsamples 124 most-common words\n",
            "2018-11-03 21:11:10,041 : INFO : downsampling leaves estimated 408334 word corpus (144.1% of prior 283362)\n",
            "2018-11-03 21:11:10,218 : INFO : estimated required memory for 8382 words and 100 dimensions: 10896600 bytes\n",
            "2018-11-03 21:11:10,219 : INFO : updating layer weights\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1GyQ10z52S4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d7cc1ae8-9971-44fb-ef28-66367a989dab"
      },
      "cell_type": "code",
      "source": [
        "# Re-train (you need to specify total_examples) \n",
        "# we only have ~25k tweets...and it shouldnt take long\n",
        "\n",
        "model_toxic.train(data_hatespeech['tokenized'], total_examples = model_toxic.corpus_count, epochs=2)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 21:12:38,380 : WARNING : Effective 'alpha' higher than previous training cycles\n",
            "2018-11-03 21:12:38,382 : INFO : training model with 4 workers on 62589 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2018-11-03 21:12:38,792 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-11-03 21:12:38,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 21:12:38,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 21:12:38,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 21:12:38,816 : INFO : EPOCH - 1 : training on 305428 raw words (219843 effective words) took 0.4s, 524751 effective words/s\n",
            "2018-11-03 21:12:39,207 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-11-03 21:12:39,220 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-03 21:12:39,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-03 21:12:39,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-03 21:12:39,228 : INFO : EPOCH - 2 : training on 305428 raw words (219816 effective words) took 0.4s, 556892 effective words/s\n",
            "2018-11-03 21:12:39,229 : INFO : training on a 610856 raw words (439659 effective words) took 0.8s, 519834 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(439659, 610856)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "dT8a5VwpbIdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "16842b04-e561-44b4-b97d-35da5a28542e"
      },
      "cell_type": "code",
      "source": [
        "# Any new linguistic insights\n",
        "\n",
        "model_toxic.wv.most_similar('idiot')\n",
        "\n",
        "# Some of the words changed place but other than that...no big changes"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-03 21:13:46,429 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('asshole', 0.8278401494026184),\n",
              " ('moron', 0.7025822401046753),\n",
              " ('imbecile', 0.7006790637969971),\n",
              " ('fool', 0.6777839660644531),\n",
              " ('illiterate', 0.6400813460350037),\n",
              " ('loser', 0.6324371099472046),\n",
              " ('ignorant', 0.6284880042076111),\n",
              " ('retard', 0.628136396408081),\n",
              " ('hypocrite', 0.6253594756126404),\n",
              " ('insult', 0.6170707941055298)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "wUAl-DzlfiSQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Finally, we can train a fastText model (thanks Facebook for that)\n",
        "\n",
        "This will be ~4 times slower than Word2Vec (since the algo has to account for all the sub-word-stuff etc.)"
      ]
    },
    {
      "metadata": {
        "id": "ZWjwf0ECfXYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's bring all of the training data together\n",
        "all_hate_data = pd.concat([toxic_data, data_hatespeech['tokenized']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MhHc3STmc_FX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_toxic_fasttext = FastText(all_hate_data, size=100, window=5, min_count=5, workers=4, iter=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CA2cqTjHf2Ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "5cc8ea18-7183-45f5-c54d-0f26559a5d97"
      },
      "cell_type": "code",
      "source": [
        "# Let's see what we get\n",
        "\n",
        "model_toxic_fasttext.wv.most_similar('idiot', topn=20)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('idiota', 0.9229915142059326),\n",
              " ('idiom', 0.9093338847160339),\n",
              " ('idi', 0.8576018810272217),\n",
              " ('idioma', 0.855239987373352),\n",
              " ('idiocy', 0.8424909710884094),\n",
              " ('iot', 0.7994241714477539),\n",
              " ('asshole', 0.7702264189720154),\n",
              " ('idiotic', 0.7408390045166016),\n",
              " ('idw', 0.7392857074737549),\n",
              " ('idc', 0.7311899662017822),\n",
              " ('ediot', 0.7287427186965942),\n",
              " ('assh', 0.7223196029663086),\n",
              " ('idk', 0.7221118807792664),\n",
              " ('idiots', 0.7206251621246338),\n",
              " ('idol', 0.7098889350891113),\n",
              " ('assholery', 0.7077001333236694),\n",
              " ('idoit', 0.7054649591445923),\n",
              " ('moron', 0.7014188766479492),\n",
              " ('idf', 0.6957465410232544),\n",
              " ('idiotism', 0.6914752721786499)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "metadata": {
        "id": "cVZhH9Ueiqo_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected, fastText gives us a mix of semanticly similar and similar sounding words."
      ]
    }
  ]
}