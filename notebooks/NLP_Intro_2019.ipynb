{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "NLP_Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RJuro/Africalics-PhD-Academy-2018/blob/master/notebooks/NLP_Intro_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rkI7PkFoVla",
        "colab_type": "text"
      },
      "source": [
        "# Intro to Natural Language Processing \n",
        "\n",
        "The second part of M2 will be dealing with language data â€“ working our way from simple string manipulation to word- and document-embeddings wit [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)(Mikolov et al. 2013) and [FastText](https://fasttext.cc/). \n",
        "While NLP before 2013/2014 would have been very much focused on comutational linguistics, the introduction word-embeddings (or word-vectors) triggered a revolution in this field and let to a retreat of grammar/rule-based approaches.\n",
        "\n",
        "![alt text](https://rawcdn.githack.com/SDS-AAU/M2-2019/c8b4c787e1ee8e5015c661db6305c9529e988ec6/notebooks/goldberg_nlp_hist.png)\n",
        "source: Yuval Goldberg: https://youtu.be/e12danHhlic?t=428\n",
        "\n",
        "This is for instance reflected in the [course design](http://web.stanford.edu/class/cs224n/) at top institutions where NLP courses have been merged with Deep Learning courses and many linguistic approaches have vanished from the curriculum.\n",
        "\n",
        "The SDS-NLP curriculum will be covering the most central basics in a very broad fashion to provide you with an overview of the different concepts and approaches in this discipline. The goal will be to enable you to integrate text data into unsupervised and supervised ML projects.\n",
        "\n",
        "We will start with simple string manipulation and preprocessing, then move to (still relevant) old-school tasks such as part-of-speech tagging, and to \"simple\" nummerical text representation. The curriculum will end with an introduction to language-models & word-embeddings and how we can use them for generating rich document representations in ML pipelins.\n",
        "\n",
        "Being able to laverage NLP techniques will enable to to synthesize data out of abundand unstructured text. It is a skill/discipline that is not often instructed because of its too \"soft\" character (for CS people) and therefore organisations - even tech giants - are often not up to date when it comes to using NLP.\n",
        "\n",
        "Aside from the Datacamp courses there are some great resources on NLP online.\n",
        "\n",
        "- Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \" O'Reilly Media, Inc.\".\n",
        "https://www.nltk.org/book/\n",
        "\n",
        "- Silge, J., & Robinson, D. (2017). Text mining with R: A tidy approach. \" O'Reilly Media, Inc.\". https://www.tidytextmining.com/\n",
        "\n",
        "\n",
        "- Stanford's NLP online course (by Dan Jurafsky and Chris Manning):\n",
        "https://www.youtube.com/watch?v=3Dt_yh1mf_U&list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm (playlist), \n",
        "\n",
        "- Stanford's NLP & Deep Learning course. https://youtu.be/8rXD5-xhemo (first video - total must see because Chris Manning is amazing)\n",
        "\n",
        "\n",
        "- If you cannot get enough (especially around November when we are in M3):\n",
        "\n",
        "- Goldberg, Y. (2017). Neural network methods for natural language processing. Synthesis Lectures on Human Language Technologies, 10(1), 1-309. (recent presentation of Yuval at SpaCy IRL: https://youtu.be/e12danHhlic)\n",
        "\n",
        "- (Advanced) NLP course at ABBYY - the first 5 notebooks are already translated from Russian. (Format similar to SDS)\n",
        "https://github.com/DanAnastasyev/DeepNLP-Course/blob/master/README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfjEk_oJoVlb",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title ## Some Youtube Context\n",
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('3Dt_yh1mf_U',start=435, frameborder=\"0\", width=800, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtg0fI7toVlf",
        "colab_type": "text"
      },
      "source": [
        "In this Notebook, we will attept to cover the most necessary (imo) basics of Natural Language Processing that will be a working foundation for your future data-mining and ML projects.\n",
        "\n",
        "We will regard NLP (or more specifically text mining) as a tool for structuring information that is embedded in text-data. Overall, we hope that we can use the computer to read text for us and extract interesting elements from it.\n",
        "\n",
        "This notebook will guide you through:\n",
        "\n",
        "- Simple string manipulation\n",
        "- The concept of tokens and tokenization\n",
        "- Part of speech tagging (POS)\n",
        "- Lemmatization and stemming\n",
        "- Named entity extraction - and linking up in networks\n",
        "- Bag-of-words model & Tf-Idf\n",
        "- Unsupervised Learning with NLP: LSA, LDA\n",
        "\n",
        "\n",
        "Generally, in NLP that aims at ML, we should distinguish between two (ideal) cases:\n",
        "\n",
        "1. We have few long texts\n",
        "2. We have many short texts\n",
        "\n",
        "In most situations where we try to model specific meaning structures it is the 2. case that is most appropriate. However, there are ways to deal with the first case, too.\n",
        "\n",
        "Along the way, I will try to incorporate some useful tools to get text-data and other tricks...\n",
        "\n",
        "### R-people:\n",
        "\n",
        "- [Quanteda](https://quanteda.io/)\n",
        "- [tidytext](https://www.tidytextmining.com/)\n",
        "- [text2vec](http://text2vec.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjn8wbf1oVlg",
        "colab_type": "text"
      },
      "source": [
        "### Simple String Manipulation\n",
        "\n",
        "We start by taking a piece of text and turning it into something that carries the meaning of the initial text but is less noisy and thus perhaps easier to \"understand\" by a computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjS20URioVlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"The Eton-educated, non-binary British Iraqi had always struggled with their identity, until they discovered drag. Yet the 29 year old says the performances come at a high price\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSPpEsXwoVlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split on fullstop\n",
        "text.lower().split(\".\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8wzPgqAoVlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split on empty space\n",
        "text.split(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ba_2LOoVlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find in text (position)\n",
        "text.find('29')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P391O7JoVlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple replacement\n",
        "text.replace('o', 'O')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiNk7YmWoVlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# very short RegEx\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvidsjTloVlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re.findall(r'\\d+', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBB8xJVYoVlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numbers = re.findall(r'\\d+', text)\n",
        "\n",
        "for i in numbers:\n",
        "    print(text.replace(i, str(int(i) + 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ImVLtHoVlz",
        "colab_type": "text"
      },
      "source": [
        "More on RegEx in the Datacamp courses (there is a whole course on that actually) and [here](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZzQsIYGrXM1",
        "colab_type": "text"
      },
      "source": [
        "Overall, in NLP we are trying to represent meaning structure. That means that we want to focus on the most important and \"meaning-bearing elements\" in text, while reducing noise.\n",
        "Words such as \"and\", \"have\", \"the\" may have central syntactic functions but are not particularly important from a semantic perspective.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlZ4knHPoVl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining stopwords\n",
        "\n",
        "stopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
        "                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
        "                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
        "                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
        "                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
        "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
        "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
        "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n",
        "                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
        "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
        "                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
        "                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
        "                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
        "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
        "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
        "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
        "                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n",
        "                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n",
        "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
        "                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
        "                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
        "                'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13czGzLmoVl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's keep only words that are not stopwords\n",
        "[word for word in text.lower().split() if word not in stopwords_en]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln9stkrdoVl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's use RegEx one more time to remove leading or trailing punctuation from our words\n",
        "'drag.,'.strip(r'[\" ,.!?:;\"]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsyI6t1-oVl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's combine that and add another condition \"No numbers\"\n",
        "[word.strip(r'[\" ,.!?:;\"]') for word in text.lower().split() if word not in stopwords_en and not word.isdigit()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whVULFm5oVl9",
        "colab_type": "text"
      },
      "source": [
        "Now that you undestand (hopefully) whatâ€™s going on on the basic level, let's start using some more sophisticated tools to work with text.\n",
        "\n",
        "We will import some tokenizers from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZOY9IcapnCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk #this part is needed on colab.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "#----------------------------------------\n",
        "\n",
        "# Tokenizing sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenizing words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wbQ7SsXoVmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's get our stences.\n",
        "# Note that the full-stops at the end of each sentence are still there\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUEFiD0qoVmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use word_tokenize to tokenize the third sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[1])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(text))\n",
        "\n",
        "print(tokenized_sent)\n",
        "print(unique_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgdJTl_5pZnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G50Jl_MrpyLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[word.lower() for word in word_tokenize(text) if word not in stop_words and word.isalnum()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFEDijtCoVmE",
        "colab_type": "text"
      },
      "source": [
        "### Processing many short texts and simple stats\n",
        "\n",
        "An introduction to NLP would not be the same without Donald's tweets. Let's use these tweets for some more basic NLP and let's try to gather some insights...maybe\n",
        "\n",
        "![donald_tweets](https://i.cdn.cnn.com/cnn/interactive/2017/politics/trump-tweets/media/trump-tweets-hdr-02.jpg)\n",
        "\n",
        "Let's try to use some very simple statistics on twitter data:\n",
        "\n",
        "thanks to [Trump Twitter Archive](http://www.trumptwitterarchive.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eC60LPQoVmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) #to see more text\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCNZoQA3oVmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing Tweets made easy!\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIcUPTayoVmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download and open some Trump tweets from trump_tweet_data_archive\n",
        "\n",
        "trump_tweets_df = pd.read_json('https://github.com/bpb27/trump_tweet_data_archive/raw/master/condensed_2018.json.zip')\n",
        "trump_tweets_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPWtWgyaoVmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index (not really needed but why not)\n",
        "trump_tweets_df = trump_tweets_df.set_index(pd.to_datetime(trump_tweets_df.created_at))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbQtp1mDoVmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing the tokenizer\n",
        "tknzr.tokenize(\"I am a very #cool tweet by @Roman\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDU9J4PCoVmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's identify people Trump likes to mention\n",
        "trump_tweets_df['mentions'] = trump_tweets_df['text'].map(lambda textline: [tag for tag in tknzr.tokenize(textline) if tag.startswith('@')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOwtYQ5aoVmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only keep tweets where a mention i present\n",
        "trump_tweets_df = trump_tweets_df[trump_tweets_df['mentions'].map(len) > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F36Hl_toVmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect\n",
        "trump_tags = itertools.chain(*trump_tweets_df['mentions'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx7l7KjXoVma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count up and show\n",
        "counted_tags = Counter(trump_tags)\n",
        "counted_tags.most_common()[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjWb3wQnemfh",
        "colab_type": "text"
      },
      "source": [
        "#### Most simple sentiment analysis\n",
        "\n",
        "let's try to categorize our trump tweets by simply matching terms\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*T_wr7iiNs9_OR8V19w0qGw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1ec3-FcMKdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a dictionary of positive and negative terms\n",
        "#!wget https://gist.githubusercontent.com/mkulakowski2/4289437/raw/1bb4d7f9ee82150f339f09b5b1a0e6823d633958/positive-words.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyDixb9Dee7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://gist.githubusercontent.com/mkulakowski2/4289441/raw/dad8b64b307cd6df8068a379079becbb3f91101a/negative-words.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgaeeUie4mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open them by figuring out where the wordlist actually starts, indexing and splitting\n",
        "\n",
        "#positive_words = open('positive-words.txt','r').read()[1540:].split('\\n')\n",
        "\n",
        "#negative_words = ...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVojGizNfk27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create count lists of positive and negative words\n",
        "\n",
        "#positive_count = trump_tweets_df['text'].map(lambda textline: len([... in tknzr.tokenize(textline) if word in positive_words]))\n",
        "#negative_count = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYKGAnEjgKTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a new column \"is_positive\" that is true where positive counts > negative counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8UngJUpP-Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check how you did by looking at some positive ones?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqb0oIy6G_rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot over time positive averages by month"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHuizQQ1PEX3",
        "colab_type": "text"
      },
      "source": [
        "Want to check out the [solution](https://colab.research.google.com/github/SDS-AAU/M2-2019/blob/master/notebooks/NLP_Quick_sentiment_analysis_exercise.ipynb)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3RhsCmBQgGT",
        "colab_type": "text"
      },
      "source": [
        "### Long texts and extracting some linguistic features\n",
        "\n",
        "Now, that we have some experience with short texts, let's try out to work with longer texts. We will be analysing a newspaper article as well as a whole (very long) book. I prepared an example which also covers data-collection as well as easy article scaping [here](https://colab.research.google.com/github/SDS-AAU/M2-2019/blob/master/notebooks/Newspaper_NER_Networks.ipynb).\n",
        "\n",
        "In this part of the tutorial, I will introduce Spacy, a high-level DeepLearning based NLP library that will help us to do complex stuff with not too much code and without havint do go deep into \"old-school-NLP\"\n",
        "\n",
        "<h2 align=\"center\"></h2>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" width=\"600\">\n",
        "\n",
        "\n",
        "Here a nice [cheat-sheet\n",
        "](https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Python/spaCy.pdf)\n",
        "Spacy is today one of the leading solutions for NLP in industry which goes as far as them hosting a [whole conference](https://www.youtube.com/watch?v=hNPwRPg9BrQ&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc) with the leading NLP experts worldwide in Berlin last summer\n",
        "\n",
        "I will introduce you to some functionality but not all. There is an advanced course on SpaCy that can be found on DC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "FHv1H72STooD",
        "colab": {}
      },
      "source": [
        "#@title ## Spacy past - present - future\n",
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('Jk9y17lvltY',start=435, frameborder=\"0\", width=800, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R_8wgJ7oVme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is a long article text \n",
        "article = \"\"\"'(CNN) Donald Trump drove Democrats to the first crucial pivot point of their impeachment confrontation on Tuesday with a defiant declaration that his administration would not cooperate with the investigation.\\n\\nIn a fierce counter-attack after days of failing to control a torrent of damaging disclosures, the Trump White House branded the inquiry an illegal bid to overthrow the 2016 election and blocked testimony from a top diplomat.\\n\\n\"Never before in our history has the House of Representatives -- under the control of either political party -- taken the American people down the dangerous path you seem determined to pursue,\" White House counsel Pat Cipollone wrote in a letter to House Speaker Nancy Pelos i and her key committee chairs.\\n\\nThe letter in effect gave notice of all out political warfare as part of the administration\\'s strategy to deprive investigators of all the testimony and evidence that they have demanded, in a clear effort to throttle the capacity of the probe into whether Trump abused his power by pressuring Ukraine to investigate a political opponent -- Joe Biden.\\n\\nPelosi says there is no constitutional requirement supporting Trump\\'s demand for a full House vote to initiate impeachment proceedings -- one justification given for the President\\'s refusal to cooperate.\\n\\nBut Trump\\'s move left her with grave strategic decisions on what to do next in a confrontation that puts to the test the integrity of America\\'s bedrock separation of powers and will determine whether she truly gamed out this duel several steps ahead.\\n\\nChallenging Trump\\'s position in court could bog down the impeachment drive in months of legal challenges. Folding the President\\'s obstruction into articles of impeachment in short order could play into his claims that she\\'s running a \"kangaroo court\" and rushing the most consequential function of Congress.\\n\\nThe American people will now be effectively asked whether a President who accepts few limits on his power can be held in check by a separate branch of government or whether he can avoid such an examination, a decision that will echo through history.\\n\\nDemocrats are already arguing that Trump\\'s position is a de facto admission of guilt based on a legal and political house of sand.\\n\\n\"I guess they haven\\'t read the Constitution,\" said Rep. Tom Malinowski of New Jersey, a Democrat who serves on the House Foreign Affairs Committee.\\n\\n\"If they don\\'t defend themselves, against the copious evidence that we already have, then I think it disadvantages them,\" Malinowski told CNN\\'s Erin Burnett.\\n\\n\"This is not the kind of investigation where we are starting with nothing -- we are starting with everything.\"\\n\\nTrump\\'s call \\'crazy\\' and \\'frightening\\'\\n\\nThe nation\\'s most serious political crisis in decades came to a head as more shocking details emerged of Trump\\'s attempt to pressure Ukraine.\\n\\nThe now famous whistleblower wrote a memo that describes a White House official as characterizing the call with Volodymyr Zelensky as \"crazy\" and \"frightening,\" a source familiar with the whistleblower complaint said.\\n\\nThe New York Times , which first reported the new details, said in its piece that White House lawyers discussed how to handle the discussion because in the official\\'s view the president had clearly committed a criminal act.\"\\n\\nIn another break in the drama, CNN reported that Gordon Sondland -- the US ambassador to the EU who Trump prevented from testifying to Congress on Tuesday -- directly called the President in September to find out what was going on amid discussions among his peers over whether US military aid to Ukraine was being withheld.\\n\\nThe nugget raises the possibility that Sondland\\'s text to a colleague that there was no quid pro quo involved was on the orders of the President himself -- a possibility Democrats will surely want to investigate.\\n\\nHouse Intelligence Committee chairman Adam Schiff suggested Monday that Sondland had texts and emails on a personal device that the State Department was refusing to hand over.\\n\\nThe new revelations explain why the White House is refusing to cooperate with the impeachment inquiry. It can\\'t allow a window into wild, self-serving and possibly even criminal behavior by the President in his dealings with Ukraine that could turn Americans against him.\\n\\nAs Democrats seek to make a case that the President defies constitutional norms and abuses his power by setting foreign policy for personal political ends, Trump\\'s aides must try to slow their momentum and weave a tale of congressional overreach.\\n\\nThe struggle intensified further after multiple polls showed a majority of Americans now support opening an impeachment inquiry.\\n\\nBut there is not yet a majority for removing the President from office, underscoring the critical impact of the political battle in Washington now being fully joined by both sides.\\n\\nTrump dares Pelosi to hold risky vote\\n\\nTrump\\'s letter effectively dares Pelosi, who may want to protect her more moderate members from political damage, to hold a full vote in the House on moving forward with the inquiry.\\n\\nSuch a vote was held in the last two impeachment sagas concerning Presidents Richard Nixon and Bill Clinton, but there is nothing in the Constitution that mandates such a vote. Trump\\'s team says the absence of such a vote means that he has no choice but to withhold cooperation to preserve the scope of his office for future occupants.\\n\\n\"At a constitutional level, that is what we call complete and total nonsense,\" said CNN legal and national security analyst Susan Hennessey on \"The Situation Room.\"\\n\\nThe White House maintained that without such a vote, the President and other Executive Branch officials will be denied basic rights available to all Americans.\\n\\nIt accused Pelosi of denying Trump the right to cross-examine witnesses, to have access to evidence, and for counsel to be present during depositions.\\n\\n\"Put simply, you seek to overturn the results of the 2016 election and deprive the American people of the President they have freely chosen,\" Cipollone wrote.\\n\\nHe also argued that the President did nothing wrong in his call with Zelensky, and claimed that Democrats had prejudiced the case with unfair process and had violated the separation of powers.\\n\\nThe tone of the letter however was far more partisan in tone than legalistic, reflecting that the battle over Trump\\'s fate will now come down to a vicious political fight. Mostly, it appeared to defend Trump based on the perceived unfairness of the political process rather than the merits of the Ukraine case.\\n\\nPelosi vowed in her own letter to House Democrats: \"The President will be held accountable. When it comes to impeachment, it is just about the facts and the Constitution,\" she wrote.\\n\\n\"At the same time as President Trump is obstructing justice, abusing power and diminishing the office of the presidency, we have a responsibility to strengthen the institution in which we serve. This is essential if we are to honor the separation of powers which is the genius of the Constitution.\"\\n\\nThe Speaker could decide to call the President\\'s bluff by scheduling a full House vote. But there is no guarantee that Trump would cooperate if a full House vote takes place.\\n\\n\"We don\\'t want to speculate on what would happen in various hypothetical situations\" Cipollone wrote.\\n\\nA vote would also give Republicans a platform to grandstand and to turn the impeachment process into a circus -- as they have done in previous Democratic oversight hearings -- a factor that might weigh on Pelosi\\'s deliberations.\\n\\nBut there is also an argument that offering Trump a blueprint for an open process, with clearly defined impeachment goals, is not just politically smart but it\\'s the right thing to do at a perilous national moment that demands basic standards of fairness.\\n\\nAs the shockwaves of Trump\\'s letter rocked Capitol Hill, Democrats issued a subpoena for testimony and evidence from Sondland, who was stopped from offering a deposition on Capitol Hill by the White House hours before his Tuesday appointment.\\n\\nStill, as this tumultuous presidency has shown, refusing a congressional subpoena is less risky than ignoring a criminal summons. Democrats could hold recalcitrant witnesses in contempt but that would entail the kind of legal imbroglio they are seeking to avoid, which Trump plans to create.\\n\\nDemocratic House Intelligence Committee Chairman Adam Schiff hinted that the remedy for the refusal of Sondland and other key officials to testify would rebound against the President.\\n\\n\"The failure to produce this witness, the failure to produce these documents we consider yet additional strong evidence of obstruction of the constitutional functions of Congress,\" Schiff told reporters.\\n\\nGirding for the battle ahead, the White House contacted outside lawyers as it seeks impeachment counsel. One of those attorneys is former South Carolina Rep. Trey Gowdy, CNN\\'s Pamela Brown reported on Tuesday.'\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI225-snxKx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's fire up spacy\n",
        "\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dZciiFC3MPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and load the small english language model. Large models can be downloaded for many languages.\n",
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Ktv5qB3O6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's apply the model to the article (as easy as that)\n",
        "article_nlp = nlp(article)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH3byTQ1XGW-",
        "colab_type": "text"
      },
      "source": [
        "When you call spaCy it will perform a number of NLP tasks on the text right away.\n",
        "These tasks are bundled in a pipeline, which can be customised if needed.\n",
        "\n",
        "![alt text](https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)\n",
        "\n",
        "source: https://spacy.io/usage/processing-pipelines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBlU9NMc7ZsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Yes spacy recognises entities in the text out of the box (among many other things)\n",
        "[(ent.text, ent.label_) for ent in article_nlp.ents][:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK8RRq_EYRLS",
        "colab_type": "text"
      },
      "source": [
        "No more Trump. Let's raise the bar with some **Fyodor Dostoevsky**\n",
        "\n",
        "![alt text](https://i.pinimg.com/564x/bc/eb/9c/bceb9cef99abbed52b940767c9530bbc.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U07_MgHDLc-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's drop wget-ting and use requests insted (like grown ups)\n",
        "\n",
        "import requests as rq \n",
        "\n",
        "crime_and_punishment_raw = rq.get('http://www.gutenberg.org/files/2554/2554-0.txt').text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LppHcs8pL97z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can devide the book up into sentences\n",
        "\n",
        "crime_and_punishment_sents = sent_tokenize(crime_and_punishment_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeY7CkAgMRPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and then feed every sentence into spacy\n",
        "crime_and_punishment_sents_nlp = [doc for doc in nlp.pipe(crime_and_punishment_sents)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-9WAiwGLij6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#And that's how sentence 200 looks like\n",
        "crime_and_punishment_sents_nlp[200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktr55DdQMzSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now a bit of a longer chunk of code\n",
        "# Let's make a list of lists with all person entities\n",
        "ents_from_sents = []\n",
        "\n",
        "for sentance in crime_and_punishment_sents_nlp:\n",
        "    \n",
        "  sentance_entities = [ent.text for ent in sentance.ents if ent.label_ == 'PERSON']\n",
        "  sentance_entities = list(set(sentance_entities))\n",
        "\n",
        "  if len(sentance_entities) > 1:\n",
        "    ents_from_sents.append(sentance_entities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2P4kK59M_7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not perfect but I guess OK...\n",
        "ents_from_sents[10:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOOo3F-ef1xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And here is what we are going to do with it\n",
        "# Yes - a network\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSlwq-n-NMs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate graph\n",
        "G = nx.Graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zsXoZlXNOk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the graph\n",
        "for i in ents_from_sents:\n",
        "  G.add_edges_from(list(itertools.combinations(i,2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IavoDfRNT5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate degre-centrality and assign it as a node-attribute\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "nx.set_node_attributes(G, degree_centrality, 'degree')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gja1017gNukO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting a \"reasonable\" lower bound.\n",
        "perc_filter = np.percentile([v for u,v in degree_centrality.items()], 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpGK-PRyNVfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a subgraph based on nodes with a degree_centrality over the threshold\n",
        "nodes_selected = [x for x,y in degree_centrality.items() if y >= perc_filter]\n",
        "\n",
        "G = G.subgraph(nodes_selected)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycs3PenkOB7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not pretty at all\n",
        "plt.figure(figsize=(12,12)) \n",
        "nx.draw_kamada_kawai(G, with_labels = True, node_size=[v * 100 for v in dict(G.degree).values()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o1PFnBfhA3J",
        "colab_type": "text"
      },
      "source": [
        "want to know more? Read the book or check out this: https://www.sparknotes.com/lit/crime/characters/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf1JFlnmaSUB",
        "colab_type": "text"
      },
      "source": [
        "### Bag of words model\n",
        "\n",
        "In order for a computer to understand text we need to somehow find a useful representation.\n",
        "If you need to compare different texts e.g. articles, you will probably go for keywords. These keywords may come from a keyword-list with for example 200 different keywords\n",
        "In that case you could represent each document with a (sparse) vector with 1 for \"keyword present\" and 0 for \"keyword absent\"\n",
        "We can also get a bit more sophoistocated and count the number of times a word from our dictionary occurs.\n",
        "For a corpus of documents that would give us a document-term matrix\n",
        "![example](https://i.stack.imgur.com/C1UMs.png)\n",
        "\n",
        "Let's try creating a bag of words model from our initial example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64CPbz3xaWpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    \n",
        "     'A text about cats.',\n",
        "     'A text about dogs.',\n",
        "     'And another text about a dog.',\n",
        "     'Why always writing about cats and dogs?',\n",
        "   ]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pCTn2y6a2ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(X.A, columns=vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roRMrE7HpNHp",
        "colab_type": "text"
      },
      "source": [
        "#### TF-IDF - Term Frequency - Inverse Document Frequency\n",
        "\n",
        "A token is importan for a document if appears very often\n",
        "A token becomes less important for comparaison across a corpus if it appears all over the place in the corpus\n",
        "\n",
        "*Innovation* in a corpus of abstracts talking about innovation is not that important\n",
        "\n",
        "\\begin{equation*}\n",
        "w_{i,j} = tf_{i,j}*log(\\frac{N}{df_i})\n",
        "\\end{equation*}\n",
        "\n",
        "- $w_{i,j}$ = the TF-IDF score for a term i in a document j\n",
        "- $tf_{i,j}$ = number of occurence of term i in document j\n",
        "- $N$ = number of documents in the corpus\n",
        "- $df_i$ = number of documents with term i\n",
        "\n",
        "\n",
        "We will use TF-IDF to transform our corpus. However, first we need to fir the TF-IDF model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2TjxKuHnEzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDln35hvo7qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(X.A, columns=vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA4te7UU3SP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One more time trump\n",
        "# sspaCy also splits sentences\n",
        "[sentance for sentance in article_nlp.sents][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3m4lIm44qv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And: it will also annotate them with POS-labels\n",
        "sentance_2 = [sentance for sentance in article_nlp.sents][2]\n",
        "[(token.text, token.pos_) for token in sentance_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou1jGUv74843",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use this to extract only tokens that we think bear most of the meaning\n",
        "[token.text for token in sentance_2 if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] and not token.is_stop] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFiFOnvEW5Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Also, we can create lemmas, thus reducing heterogeneity in the vocabulary without sacrificing much meaning\n",
        "\n",
        "[tok.lemma_ for tok in nlp(\"a text about innovations and all kinds things\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvo7rxqOTIF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Isn't that great?\n",
        "\n",
        "[token.lemma_.lower() for token in sentance_2 if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] and not token.is_stop] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ33e5O8ic-c",
        "colab_type": "text"
      },
      "source": [
        "Thus we have created a representation of a text that is probably as \"minimal\" as possible - Maximising meaning and minimising \"moise\"\n",
        "\n",
        "In the last part of this notebook we will try to use such representations to explore the content of text collections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHV7quvyL0cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's fist install this nice visualizer\n",
        "!pip install pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuGZe88uMxCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and import it\n",
        "import pyLDAvis.gensim\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmAhAMcrjJjU",
        "colab_type": "text"
      },
      "source": [
        "We will be using a dataset from EU Cordis which describes H2020 research projects. No tweets for now.\n",
        "\n",
        "http://data.europa.eu/euodp/en/data/dataset/cordisH2020projects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZyun0T2US-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I put the data on our SDS server to make things faster (it takes forever to lode otherwise)\n",
        "\n",
        "reports = pd.read_csv('http://sds-datacrunch.aau.dk/public/data/cordis-h2020reports.csv', sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5aD1YE3dHNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reports.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF9cVYcNakPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to speed things up, we will keep the data at 500 projects\n",
        "# create a random sample\n",
        "\n",
        "reports = reports.sample(500)\n",
        "reports = reports[reports['language'] == 'en']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLXIRn3Pl3_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reindec\n",
        "reports.index = range(len(reports))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9ZYojgcX8xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now, let's combine everything that we learned about preprocessing in a few lines of code\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for tweet in nlp.pipe(reports['summary']):\n",
        "  proj_tok = [token.lemma_.lower() for token in tweet if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] and not token.is_stop] \n",
        "  tokens.append(proj_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swYK2bBvbbol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's bring the tokens back in\n",
        "\n",
        "reports['tokens'] = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPG-IPHRkkV9",
        "colab_type": "text"
      },
      "source": [
        "Another library that you have to know when doing NLP (once you progress to DeepLearning and recent stuff probably not any more but for now) is gensim\n",
        "\n",
        "https://radimrehurek.com/gensim/\n",
        "\n",
        "This is the library that handles all kinds of statistical NLP tasks and goes as far as implementing (super efficient) embedding model training (next class)\n",
        "But: With NLP today being all BERT, ELMO and transformers probably declining in importance. Back in 2013 gensim was a major discovery and breakthrough helper when I was working on my PhD. One more reason to have a look at it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWU7zITlkwPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the dictionary builder\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmNmWMvZ8Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(reports['tokens'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZW6Idn_btnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB6aB8E-bkW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in reports['tokens']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNOI_-xNl5PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus[100][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4krg3HfmAhp",
        "colab_type": "text"
      },
      "source": [
        "### Topic modelling\n",
        "\n",
        "The corpus is a list of tuples, with word-ids and the number of their occurrence in documents: LDA - https://youtu.be/DWJYZq_fQ2A\n",
        "\n",
        "We will start with a topic modelling approach that is good for interpretable topics but not too much for further processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3vcFRRqbsFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we'll use the faster multicore version of LDA\n",
        "\n",
        "from gensim.models import LdaMulticore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv_Z6JXIb526",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model (makes some mess atm due to version clashes)\n",
        "\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary,  num_topics=10, workers = 2, passes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXPrQHO9nDhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check out topics\n",
        "lda_model.print_topics(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDIH34Lvb7Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try to visualize\n",
        "lda_display = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, sort_topics=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmDFJBBFcBHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80U1-IrnK77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Where does text one belong to?\n",
        "lda_model[corpus][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYOPWeMLpMcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And that's how you get the topic-number that's ranked highest\n",
        "\n",
        "sorted([(2, 0.121567), (9, 0.8610384)], key=lambda x: -x[1])[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A6aJYDXoorZ",
        "colab_type": "text"
      },
      "source": [
        "From here, you can assign topics to texts...do some EDA, explore how topics evolve over time etc.\n",
        "\n",
        "Finally, let's try out LSA (an older topic-moddeling approach similar to NMF) - thus unsupervised ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIc4Nd0SqZ7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the TfidfModel from Gensim\n",
        "from gensim.models.tfidfmodel import TfidfModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ABvkfSjd4ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and fit a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_geBUz3eJ8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can transform the whole corpus\n",
        "tfidf_corpus = tfidf[corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k36xISjteOkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just like before, we import the model\n",
        "from gensim.models.lsimodel import LsiModel\n",
        "\n",
        "# And we fir it on the tfidf_corpus pointing to the dictionary as reference and the number of topics.\n",
        "# In more serious settings one would pick between 300-400\n",
        "lsi = LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDMZkYbCeTda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lsi.show_topics(num_topics=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj_nxOcveWuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And just as before, we can use the trained model to transform the corpus\n",
        "lsi_corpus = lsi[tfidf_corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpyEfcy9eeXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the MatrixSimilarity\n",
        "from gensim.similarities import MatrixSimilarity\n",
        "\n",
        "# Create the document-topic-matrix\n",
        "document_topic_matrix = MatrixSimilarity(lsi_corpus)\n",
        "document_topic_matrix_ix = document_topic_matrix.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI8K24W2lTcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this now allows us to perform similarity-queries\n",
        "\n",
        "sims = document_topic_matrix[lsi_corpus[0]]\n",
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "print(sims)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvNB5tGWrtEi",
        "colab_type": "text"
      },
      "source": [
        "We will go deeper into how that works next time\n",
        "The last bit is a bit of a quick bonus and should be super familiar from M1.\n",
        "\n",
        "Since we now have a matrix with observations and features - why not trying to apply unsupervisd ML that we know from M1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBn8C2lIefgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimensionali lity reduction for plotting\n",
        "import umap\n",
        "embeddings = umap.UMAP(n_neighbors=15, metric='cosine').fit_transform(document_topic_matrix_ix)\n",
        "\n",
        "#------------------------------\n",
        "# we could use that too\n",
        "\n",
        "#from sklearn.decomposition import PCA\n",
        "\n",
        "#reduced = PCA(n_components = 10).fit_transform(document_topic_matrix_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlDGWflf5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nothing new here\n",
        "from sklearn.cluster import KMeans\n",
        "clusterer = KMeans(n_clusters = 10)\n",
        "clusterer.fit(document_topic_matrix_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9y77tBVe3VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting things\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "plt.figure(figsize=(12,12))\n",
        "g = sns.scatterplot(*embeddings.T,\n",
        "                    #reduced[:,0],reduced[:,1],\n",
        "                   hue=clusterer.labels_,\n",
        "                    palette=\"Paired\",\n",
        "                   legend='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQYukJYIgYwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's explore the clusters ... that should actually correlate with topics found by LDA\n",
        "reports['cluster'] = clusterer.labels_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3chqB3Sg-8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reports[reports['cluster'] == 2]['teaser']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}